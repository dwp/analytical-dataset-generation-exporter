# DO NOT USE THIS REPO - MIGRATED TO GITLAB

# analytical-dataset-generation-exporter

## Custom spark metric exporter for the Analytical Dataset Generator

The release JAR from this project is intended to be used with the [analytical-dataset-generation](https://github.com/dwp/aws-analytical-dataset-generation) repository.

The Spark job in the `analytical-dataset-generator` starts up with the release from this repository. This JAR then collects spark metrics and other custom metrics which will be pushed to the `adg-pushgateway`. This allows us to preserve those metrics even after the cluster has terminated.  

## How does this exporter get released? 

The main github actions workflow creates a github release, which is a compiled JAR. 

## How does the analytical-dataset-generation cluster fetch it

The analytical-dataset-generation(ADG) cluster fetches this repository as a [resource from concourse.](https://github.com/dwp/aws-analytical-dataset-generation/blob/f8a7db4c95f22e1d95db45193b5a50454a64b78f/ci/resources.yml#L93-L100)

    - name: analytical-dataset-generation-exporter-release
      type: github-release
      source:
        owner: dwp
        repository: analytical-dataset-generation-exporter
      check_every: 1h
      webhook_token: ((dataworks.concourse_github_webhook_token))

The terraform code in the ADG repository then [pushes it to an S3 Bucket](https://github.com/dwp/aws-analytical-dataset-generation/blob/f8a7db4c95f22e1d95db45193b5a50454a64b78f/bootstrap_actions.tf#L181-L186)

    resource "aws_s3_bucket_object" "metrics_jar" {
      bucket     = data.terraform_remote_state.common.outputs.config_bucket.id
      kms_key_id = data.terraform_remote_state.common.outputs.config_bucket_cmk.arn
      key        = "component/analytical-dataset-generation/metrics/adg-exporter.jar"
      content    = filebase64("${var.analytical_dataset_generation_exporter_jar.base_path}/analytical-dataset-generation-exporter-${var.analytical_dataset_generation_exporter_jar.version}.jar")
    }

When the ADG cluster begins bootstrapping, it [downloads this JAR.](https://github.com/dwp/aws-analytical-dataset-generation/blob/f8a7db4c95f22e1d95db45193b5a50454a64b78f/bootstrap_actions/metrics-setup.sh#L19)  
It then [places it into the dependencies path for ADG](https://github.com/dwp/aws-analytical-dataset-generation/blob/f8a7db4c95f22e1d95db45193b5a50454a64b78f/bootstrap_actions/metrics-setup.sh#L48)

## How does it get launched 

When the spark-job launches on the ADG cluster it loads up the release JAR as a dependency in [the configuration file for spark](https://github.com/dwp/aws-analytical-dataset-generation/blob/master/cluster_config/configurations.yaml.tpl,) which initiates it and the JAR begins its work. The properties for the spark metrics are setup in the [metrics.properties file](https://github.com/dwp/aws-analytical-dataset-generation/blob/master/bootstrap_actions/metrics_config/metrics.properties) and the [prometheus config](https://github.com/dwp/aws-analytical-dataset-generation/blob/master/bootstrap_actions/metrics_config/prometheus_config.yml) is nearby. 
The metrics properties are [loaded in the spark-job](https://github.com/dwp/aws-analytical-dataset-generation/blob/f8a7db4c95f22e1d95db45193b5a50454a64b78f/steps/generate_dataset_from_htme.py#L1052) to override any defaults. 

Once the JAR is up and running alongside the spark job it begins to scrape the custom metrics and push them to the configured pushgateway. 

## What metrics does it collect and how 

New metrics are created and registered in [CollectionExporter.java](https://github.com/dwp/analytical-dataset-generation-exporter/blob/master/src/main/java/org/apache/spark/metrics/source/adg/exporter/CollectionExporter.java). 
For example we can see the `input_collection_size` metric registered as: 
    `metricRegistry.register(collection + "_input_collection_size", InputCollectionSizeGauge);`
The metric and its properties such as where to find it are defined in the [InputCollectionSizeGauge.java](https://github.com/dwp/analytical-dataset-generation-exporter/blob/master/src/main/java/org/apache/spark/metrics/source/adg/exporter/InputCollectionSizeGauge.java) and it is told to find the specific metrics on the cluster: `private static String COLLECTION_SIZE_FILE = "/opt/emr/metrics/htme_collection_size.csv";` 

This Metric is actually a custom metric generated by the spark-job, which creates a file and [adds the metrics to it dynamically](https://github.com/dwp/aws-analytical-dataset-generation/blob/f8a7db4c95f22e1d95db45193b5a50454a64b78f/steps/generate_dataset_from_htme.py#L455). 
The Jar scrapes this file and pushes these metrics to the prometheus pushgateway. The metrics can be queried in [thanos-query](https://thanos-query.monitoring.mgt-dev.dataworks.dwp.gov.uk/)
 
